# Ollama ë…¸íŠ¸ë¶ ì„¤ì¹˜ ê°€ì´ë“œ

## 1. Ollama ì„¤ì¹˜

### Linux (Ubuntu/Debian)
```bash
# Ollama ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜
curl -fsSL https://ollama.com/install.sh | sh

# ë˜ëŠ” ì§ì ‘ ì„¤ì¹˜
curl -L https://ollama.com/download/ollama-linux-amd64 -o /tmp/ollama
chmod +x /tmp/ollama
sudo mv /tmp/ollama /usr/local/bin/

# Ollama ì„œë¹„ìŠ¤ ì‹œì‘
ollama serve
```

### macOS
```bash
# Homebrewë¥¼ ì‚¬ìš©í•œ ì„¤ì¹˜
brew install ollama

# ë˜ëŠ” ê³µì‹ ì¸ìŠ¤í†¨ëŸ¬ ë‹¤ìš´ë¡œë“œ
# https://ollama.com/download/mac
```

### Windows
1. ê³µì‹ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë‹¤ìš´ë¡œë“œ: https://ollama.com/download/windows
2. ì„¤ì¹˜ íŒŒì¼ ì‹¤í–‰ ë° ì„¤ì¹˜ ì™„ë£Œ

## 2. Ollama ì„œë¹„ìŠ¤ ì‹œì‘

### ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ (Linux)
```bash
# Systemd ì„œë¹„ìŠ¤ë¡œ ì‹¤í–‰ (ì¶”ì²œ)
sudo systemctl enable ollama
sudo systemctl start ollama

# ë˜ëŠ” ì§ì ‘ ì‹¤í–‰
ollama serve &

# ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
systemctl status ollama
```

### í¬íŠ¸ ë³€ê²½ (ì„ íƒì‚¬í•­)
ê¸°ë³¸ í¬íŠ¸ëŠ” 11434ì…ë‹ˆë‹¤. ë‹¤ë¥¸ í¬íŠ¸ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´:
```bash
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export OLLAMA_HOST=0.0.0.0:11434

# ë˜ëŠ” systemd ì„œë¹„ìŠ¤ íŒŒì¼ ìˆ˜ì •
sudo nano /etc/systemd/system/ollama.service
# Environment="OLLAMA_HOST=0.0.0.0:11434" ì¶”ê°€
sudo systemctl daemon-reload
sudo systemctl restart ollama
```

## 3. ì½”ì¸ ê±°ë˜ ìµœì í™” ëª¨ë¸ ì„¤ì¹˜

### ì¶”ì²œ ëª¨ë¸ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# ì½”ì¸ ê±°ë˜ ìµœì í™” Ollama ëª¨ë¸ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸

echo "=========================================="
echo "ì½”ì¸ ê±°ë˜ ìµœì í™” Ollama ëª¨ë¸ ì„¤ì¹˜ ì‹œì‘"
echo "=========================================="
echo ""

# 1. Llama 3.1 8B (ê°€ì¥ ì¶”ì²œ)
echo "1ï¸âƒ£ Llama 3.1 8B ì„¤ì¹˜ ì¤‘..."
ollama pull llama3.1:8b
if [ $? -eq 0 ]; then
    echo "âœ… Llama 3.1 8B ì„¤ì¹˜ ì™„ë£Œ"
else
    echo "âŒ Llama 3.1 8B ì„¤ì¹˜ ì‹¤íŒ¨"
fi
echo ""

# 2. DeepSeek-R1 7B (ìˆ˜ì¹˜ ë¶„ì„ ê°•ì )
echo "2ï¸âƒ£ DeepSeek-R1 7B ì„¤ì¹˜ ì¤‘..."
ollama pull deepseek-r1:7b
if [ $? -eq 0 ]; then
    echo "âœ… DeepSeek-R1 7B ì„¤ì¹˜ ì™„ë£Œ"
else
    echo "âŒ DeepSeek-R1 7B ì„¤ì¹˜ ì‹¤íŒ¨"
fi
echo ""

# 3. Qwen2.5 7B (ë²”ìš© ë²„ì „)
echo "3ï¸âƒ£ Qwen2.5 7B (ë²”ìš©) ì„¤ì¹˜ ì¤‘..."
ollama pull qwen2.5:7b
if [ $? -eq 0 ]; then
    echo "âœ… Qwen2.5 7B ì„¤ì¹˜ ì™„ë£Œ"
else
    echo "âŒ Qwen2.5 7B ì„¤ì¹˜ ì‹¤íŒ¨"
fi
echo ""

# 4. Mistral 7B Instruct (ì„ íƒì‚¬í•­)
echo "4ï¸âƒ£ Mistral 7B Instruct ì„¤ì¹˜ ì¤‘..."
ollama pull mistral:7b-instruct
if [ $? -eq 0 ]; then
    echo "âœ… Mistral 7B Instruct ì„¤ì¹˜ ì™„ë£Œ"
else
    echo "âŒ Mistral 7B Instruct ì„¤ì¹˜ ì‹¤íŒ¨"
fi
echo ""

echo "=========================================="
echo "ì„¤ì¹˜ ì™„ë£Œ! ë‹¤ìŒ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:"
echo "=========================================="
echo "  ğŸ“¦ llama3.1:8b"
echo "  ğŸ“¦ deepseek-r1:7b"
echo "  ğŸ“¦ qwen2.5:7b"
echo "  ğŸ“¦ mistral:7b-instruct"
echo ""
echo "ì„¤ì¹˜ëœ ëª¨ë¸ í™•ì¸:"
ollama list
```

### ê°œë³„ ëª¨ë¸ ì„¤ì¹˜

```bash
# Llama 3.1 8B (ê°€ì¥ ì¶”ì²œ)
ollama pull llama3.1:8b

# DeepSeek-R1 7B (ìˆ˜ì¹˜ ë¶„ì„ ê°•ì )
ollama pull deepseek-r1:7b

# Qwen2.5 7B (ë²”ìš©)
ollama pull qwen2.5:7b

# Mistral 7B Instruct
ollama pull mistral:7b-instruct

# ë” í° ëª¨ë¸ (ì„±ëŠ¥ ìš°ìˆ˜, ëŠë¦¼)
ollama pull llama3.1:70b
ollama pull qwen2.5:14b
```

## 4. ì„¤ì¹˜ í™•ì¸

```bash
# ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡ í™•ì¸
ollama list

# ëª¨ë¸ í…ŒìŠ¤íŠ¸
ollama run llama3.1:8b "ì•”í˜¸í™”í ê±°ë˜ì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•´ì¤˜"

# Ollama ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
curl http://localhost:11434/api/tags
```

## 5. ë„¤íŠ¸ì›Œí¬ ì„¤ì • (ì›ê²© ì ‘ê·¼ í—ˆìš©)

### ë°©í™”ë²½ ì„¤ì • (Linux)
```bash
# UFW ì‚¬ìš© ì‹œ
sudo ufw allow 11434/tcp

# ë˜ëŠ” iptables ì‚¬ìš© ì‹œ
sudo iptables -A INPUT -p tcp --dport 11434 -j ACCEPT
sudo iptables-save
```

### Ollama í˜¸ìŠ¤íŠ¸ ë°”ì¸ë”© í™•ì¸
```bash
# í™˜ê²½ ë³€ìˆ˜ í™•ì¸
echo $OLLAMA_HOST

# ëª¨ë“  ì¸í„°í˜ì´ìŠ¤ì— ë°”ì¸ë”© (ì›ê²© ì ‘ê·¼ í—ˆìš©)
export OLLAMA_HOST=0.0.0.0:11434

# ì„œë¹„ìŠ¤ ì¬ì‹œì‘
sudo systemctl restart ollama
```

## 6. ì½”ë“œì—ì„œ ë…¸íŠ¸ë¶ Ollama ì‚¬ìš© ì„¤ì •

### í˜„ì¬ ì„œë²„ ì„¤ì •
```python
# upbit_bot/strategies/ai_market_analyzer.py
OLLAMA_BASE_URL = "http://100.98.189.30:11434"  # ì›ê²© ì„œë²„
OLLAMA_MODEL = "qwen2.5-coder:7b"
```

### ë…¸íŠ¸ë¶ì—ì„œ ì‹¤í–‰í•˜ëŠ” ê²½ìš°
```python
# upbit_bot/strategies/ai_market_analyzer.py
OLLAMA_BASE_URL = "http://localhost:11434"  # ë¡œì»¬ ë…¸íŠ¸ë¶
OLLAMA_MODEL = "llama3.1:8b"  # ì¶”ì²œ ëª¨ë¸
```

### í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì • (ê¶Œì¥)
```bash
# .env íŒŒì¼ ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜
export OLLAMA_BASE_URL="http://localhost:11434"
export OLLAMA_MODEL="llama3.1:8b"
```

ì½”ë“œì—ì„œ í™˜ê²½ ë³€ìˆ˜ ì½ê¸°:
```python
import os

OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "llama3.1:8b")
```

## 7. ë¹ ë¥¸ ì‹œì‘ ëª…ë ¹ì–´

```bash
# 1. Ollama ì„¤ì¹˜ (Linux)
curl -fsSL https://ollama.com/install.sh | sh

# 2. Ollama ì„œë¹„ìŠ¤ ì‹œì‘
ollama serve &

# 3. ì¶”ì²œ ëª¨ë¸ ì„¤ì¹˜
ollama pull llama3.1:8b
ollama pull deepseek-r1:7b
ollama pull qwen2.5:7b

# 4. ì„¤ì¹˜ í™•ì¸
ollama list

# 5. í…ŒìŠ¤íŠ¸
ollama run llama3.1:8b "ì•ˆë…•í•˜ì„¸ìš”"
```

## 8. íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### Ollama ì„œë¹„ìŠ¤ê°€ ì‹œì‘ë˜ì§€ ì•ŠëŠ” ê²½ìš°
```bash
# ë¡œê·¸ í™•ì¸
journalctl -u ollama -f

# ë˜ëŠ” ì§ì ‘ ì‹¤í–‰í•˜ì—¬ ì˜¤ë¥˜ í™•ì¸
ollama serve
```

### ëª¨ë¸ ë‹¤ìš´ë¡œë“œê°€ ëŠë¦° ê²½ìš°
```bash
# ë„¤íŠ¸ì›Œí¬ ìƒíƒœ í™•ì¸
curl -I https://ollama.com

# í”„ë¡ì‹œ ì„¤ì • (í•„ìš”ì‹œ)
export http_proxy=http://proxy.example.com:8080
export https_proxy=http://proxy.example.com:8080
```

### ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜
```bash
# ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ í™•ì¸
free -h

# ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
ollama pull llama3.1:3b  # 8B ëŒ€ì‹  3B ì‚¬ìš©
```

### í¬íŠ¸ê°€ ì´ë¯¸ ì‚¬ìš© ì¤‘ì¸ ê²½ìš°
```bash
# í¬íŠ¸ ì‚¬ìš© í™•ì¸
sudo lsof -i :11434

# ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©
export OLLAMA_HOST=0.0.0.0:11435
```

## 9. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë° ì„ íƒ

| ëª¨ë¸ | í¬ê¸° | RAM í•„ìš” | ì†ë„ | ì½”ì¸ ê±°ë˜ ì í•©ë„ | ì¶”ì²œë„ |
|------|------|----------|------|------------------|--------|
| llama3.1:8b | 8B | ~8GB | ë¹ ë¦„ | â­â­â­â­â­ | ğŸ† ìµœê³  |
| deepseek-r1:7b | 7B | ~8GB | ì¤‘ê°„ | â­â­â­â­â­ | ğŸ¥ˆ ì¶”ì²œ |
| qwen2.5:7b | 7B | ~8GB | ë¹ ë¦„ | â­â­â­â­ | ğŸ¥‰ ì¢‹ìŒ |
| mistral:7b-instruct | 7B | ~8GB | ë¹ ë¦„ | â­â­â­â­ | ì¢‹ìŒ |
| llama3.1:70b | 70B | ~40GB | ëŠë¦¼ | â­â­â­â­â­ | ë¹„ì¶”ì²œ(ë„ˆë¬´ ëŠë¦¼) |

## 10. ìë™ ì‹œì‘ ì„¤ì • (ë¶€íŒ… ì‹œ ìë™ ì‹¤í–‰)

### Systemd ì„œë¹„ìŠ¤ (Linux)
```bash
# ì„œë¹„ìŠ¤ íŒŒì¼ ìƒì„±
sudo nano /etc/systemd/system/ollama.service
```

ë‹¤ìŒ ë‚´ìš© ì¶”ê°€:
```ini
[Unit]
Description=Ollama Service
After=network.target

[Service]
Type=simple
User=YOUR_USERNAME
ExecStart=/usr/local/bin/ollama serve
Restart=always
Environment="OLLAMA_HOST=0.0.0.0:11434"

[Install]
WantedBy=multi-user.target
```

```bash
# ì„œë¹„ìŠ¤ í™œì„±í™”
sudo systemctl daemon-reload
sudo systemctl enable ollama
sudo systemctl start ollama
```

## 11. ëª¨ë¸ ë³€ê²½ ë°©ë²•

ì½”ë“œì—ì„œ ëª¨ë¸ì„ ë³€ê²½í•˜ë ¤ë©´:

```python
# upbit_bot/strategies/ai_market_analyzer.py ìˆ˜ì •
OLLAMA_MODEL = "llama3.1:8b"  # ì›í•˜ëŠ” ëª¨ë¸ë¡œ ë³€ê²½
```

ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ë¡œ:
```bash
export OLLAMA_MODEL="llama3.1:8b"
```

## ì™„ë£Œ!

ë…¸íŠ¸ë¶ì— Ollamaê°€ ì„¤ì¹˜ë˜ê³  ì½”ì¸ ê±°ë˜ì— ìµœì í™”ëœ ëª¨ë¸ë“¤ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.

ë‹¤ìŒ ë‹¨ê³„:
1. ì½”ë“œì—ì„œ `OLLAMA_BASE_URL`ì„ `http://localhost:11434`ë¡œ ë³€ê²½
2. ì›í•˜ëŠ” ëª¨ë¸ë¡œ `OLLAMA_MODEL` ë³€ê²½
3. ê±°ë˜ ë´‡ ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸

